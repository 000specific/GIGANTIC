# GIGANTIC Source Data Ingestion Configuration
# AI: Claude Code | Opus 4.5 | 2026 February 13
# Human: Eric Edsinger
#
# ============================================================================
# HOW TO USE THIS FILE
# ============================================================================
#
# This workflow ingests user-provided source data into GIGANTIC.
# It reads a manifest of source file paths and:
#   1. Validates that all source files exist        -> OUTPUT_pipeline/1-output/
#   2. Hard copies data into organized directories  -> OUTPUT_pipeline/2-output/
#   3. Creates symlinks for STEP_2 access           -> OUTPUT_pipeline/3-output/
#
# After editing, run: bash RUN-ingest_sources.sh
#
# ============================================================================

# ============================================================================
# PROJECT SETTINGS (EDIT THESE)
# ============================================================================

project:
  # Your project name (used in output file naming and logs)
  # Example: "ctenophore_study", "species71", "my_project"
  name: "my_project"

  # Path to your source manifest file
  # This TSV file lists the data files to ingest (proteomes, genomes, GFFs)
  # Location: INPUT_user/source_manifest.tsv (relative to workflow root)
  source_manifest: "INPUT_user/source_manifest.tsv"

# ============================================================================
# OUTPUT SETTINGS
# ============================================================================

output:
  # Main output directory for all pipeline output
  base_dir: "OUTPUT_pipeline"

# ============================================================================
# INGESTION OPTIONS
# ============================================================================

ingestion:
  # How to handle missing source files
  # "error" - Stop with error message (recommended for research)
  # "warn"  - Continue but log warning
  missing_file_action: "error"

  # Overwrite existing files if they already exist (default: false)
  overwrite_existing: false

# ============================================================================
# SLURM SETTINGS (FOR HPC USERS)
# ============================================================================

slurm:
  # Enable SLURM job submission
  # Set to true if running on an HPC cluster
  enabled: false

  # SLURM account to charge
  account: "your_account"

  # SLURM Quality of Service
  qos: "your_qos"

  # Resource allocation
  memory: "4gb"
  time: "1:00:00"
  cpus: 1

# ============================================================================
# NOTES
# ============================================================================
#
# SOURCE MANIFEST FORMAT:
#
# Your source_manifest.tsv should be a tab-separated file with 4 columns:
#
#   genus_species    genome_path    gff_path    proteome_path
#   Homo_sapiens     /path/to/genome.fasta    /path/to/annotations.gff3    /path/to/proteome.aa
#
# Use "NA" for data types not available for a species.
#
# Paths can be:
#   - Absolute: /blue/moroz/data/genomes/species.fasta
#   - Relative: ../user_research/downloads/species.fasta
#
# WHAT THIS WORKFLOW DOES (3 steps, each with visible output):
#
#   Step 1 -> OUTPUT_pipeline/1-output/   Validate manifest (check all files exist)
#   Step 2 -> OUTPUT_pipeline/2-output/   Ingest data (hard copy files into organized dirs)
#   Step 3 -> OUTPUT_pipeline/3-output/   Create symlinks (for STEP_2 to consume)
#
# WHY HARD COPY + SYMLINK?
#
# Hard copying creates an archived record of exactly what was ingested.
# Symlinks in output_to_input provide the interface for downstream steps.
# This pattern ensures:
#   - Reproducibility: OUTPUT_pipeline archives what was used
#   - Clean interface: output_to_input/ has consistent structure
#   - Flexibility: Original files remain untouched
#
